{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_polt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fLQ30IjJZWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import average_precision_score, precision_score, roc_curve, auc,roc_auc_score,recall_score,f1_score, classification_report, confusion_matrix,accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import  precision_recall_curve,precision_recall_fscore_support\n",
        "import itertools \n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "from sklearn.utils.multiclass import unique_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7W1ZuCeJM3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def average_precision(algo,y_test_f,y_score_f,n_classes_f):\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    y_test_f  = label_binarize(y_test_f,  classes=[j for j in range(n_classes_f)])\n",
        "    y_score_f = label_binarize(y_score_f, classes=[j for j in range(n_classes_f)])\n",
        "    # For each class\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    average_precision = dict()\n",
        "    for i in range(n_classes_f):\n",
        "        precision[i], recall[i], _ = precision_recall_curve(y_test_f[:, i],y_score_f[:, i])\n",
        "        average_precision[i] = average_precision_score(y_test_f[:, i], y_score_f[:, i])\n",
        "\n",
        "    # A \"micro-average\": quantifying score on all classes jointly\n",
        "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test_f.ravel(),y_score_f.ravel())\n",
        "    average_precision[\"micro\"] = average_precision_score(y_test_f, y_score_f, average=\"micro\")\n",
        "    print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
        "          .format(average_precision[\"micro\"]))\n",
        "    fig = plt.figure()\n",
        "    plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,where='post')\n",
        "    plt.fill_between(recall[\"micro\"], precision[\"micro\"], alpha=0.2, color='b')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.title('Average precision score, micro-averaged over all classes: AP={0:0.2f}'.format(average_precision[\"micro\"]))\n",
        "    if n_classes_f==4:\n",
        "      path='./classification/'\n",
        "    else:\n",
        "      path='./classification2/'\n",
        "    fig.savefig(path+algo+'_average_precision.pdf')\n",
        "    return average_precision[\"micro\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XezvjGNfJlm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_roc_auc(algo,y_test_f,y_score_f,n_classes_f):\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    y_test_f  = label_binarize(y_test_f,  classes=[j for j in range(n_classes_f)])\n",
        "    y_score_f = label_binarize(y_score_f, classes=[j for j in range(n_classes_f)])\n",
        "    \n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    for i in range(n_classes_f):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_f[:, i], y_score_f[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_f.ravel(), y_score_f.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "    \n",
        "   \n",
        "    lw = 2\n",
        "    # Compute macro-average ROC curve and ROC area\n",
        "\n",
        "    # First aggregate all false positive rates\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes_f)]))\n",
        "\n",
        "    # Then interpolate all ROC curves at this points\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes_f):\n",
        "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "    # Finally average it and compute AUC\n",
        "    mean_tpr /= n_classes_f\n",
        "\n",
        "    fpr[\"macro\"] = all_fpr\n",
        "    tpr[\"macro\"] = mean_tpr\n",
        "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "    # Plot all ROC curves\n",
        "    #plt.figure()\n",
        "    fig = plt.figure()\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "             label='micro-average ROC curve (area = {0:0.2f})'\n",
        "                   ''.format(roc_auc[\"micro\"]),\n",
        "             color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "             label='macro-average ROC curve (area = {0:0.2f})'\n",
        "                   ''.format(roc_auc[\"macro\"]),\n",
        "             color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "    for i, color in zip(range(n_classes_f), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                 ''.format(i, roc_auc[i]))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic to multi-class')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    \n",
        "    if n_classes_f==4:\n",
        "      path='./classification/'\n",
        "    else:\n",
        "      path='./classification2/'\n",
        "    fig.savefig(path+algo+'_roc_auc.pdf')\n",
        "    return roc_auc[\"macro\"]\n",
        "\n",
        "#def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
        "def plot_confusion_matrix1(algo,cm, y_true_f, y_pred_f, classes_f,normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "    \"\"\"\n",
        "    # Compute confusion matrix\n",
        "    #cm = confusion_matrix(y_true_f, y_pred_f)\n",
        "    # Only use the labels that appear in the data\n",
        "    #y_true_f = np.array(y_true_f).astype(int)\n",
        "    #y_pred_f = np.array(y_pred_f).astype(int)\n",
        "    #classes_f = classes_f[unique_labels(y_true_f, y_pred_f)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes_f, yticklabels=classes_f,\n",
        "           #title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    #plt.tight_layout()\n",
        "    fig.tight_layout()\n",
        "    if len(classes_f)==4:\n",
        "      path='./classification/'\n",
        "    else:\n",
        "      path='./classification2/'\n",
        "    fig.savefig(path+algo+'_'+title+'_confusion_matrix.pdf')\n",
        "    return ax\n",
        "    \n",
        "def plot_confusion_matrix(algo,cm, classes_f,normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes_f))\n",
        "    plt.xticks(tick_marks, classes_f, rotation=45)\n",
        "    plt.yticks(tick_marks, classes_f)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    #plt.tight_layout()\n",
        "    fig.tight_layout()\n",
        "    if n_classes_f==4:\n",
        "      path='./classification/'\n",
        "    else:\n",
        "      path='./classification2/'\n",
        "    fig.savefig(path+algo+'_'+title+'_confusion_matrix.pdf')\n",
        "\n",
        "# define function for plotting the results of algorithms with additional lag feature\n",
        "def compute_plot_result_3(algo, time, y_true_f, y_pred_f,n_classess_f,target_namess_f,score_table):\n",
        "    \"\"\"compute, log and plot the performance for both training and test sets\"\"\"\n",
        "    #labels=[i for i in range(n_classess_f)]\n",
        "    \n",
        "    stuff = precision_recall_fscore_support(y_true_f, y_pred_f)\n",
        "    \n",
        "    print(classification_report(y_true_f, y_pred_f, target_names=target_namess_f))\n",
        "    print(\"Precision score: {}\".format(precision_score(y_true_f,y_pred_f, average=None)))\n",
        "    print(\"accuracy score: {}\".format(accuracy_score(y_true_f,y_pred_f)))\n",
        "    print(\"Recall score: {}\".format(recall_score(y_true_f,y_pred_f, average=None)))\n",
        "    print(\"F1 Score: {}\".format(f1_score(y_true_f,y_pred_f, average=None)))\n",
        "    \n",
        "    print (\"Precision is:\",stuff[0][0])\n",
        "    Precision= stuff[0][0]\n",
        "    print (\"Recall is:\",stuff[1][0])\n",
        "    Recall =stuff[1][0]\n",
        "    print (\"F1 is:\",stuff[2][0])\n",
        "    F1 =stuff[2][0]\n",
        "     # compute the performance\n",
        "    #r2_train = r2_score(y_train, pred_train)\n",
        "    accuracy      = accuracy_score(y_true_f,y_pred_f)\n",
        "    roc_auc       = plot_roc_auc(algo,y_true_f,y_pred_f,n_classess_f)\n",
        "    average_prec  = average_precision(algo,y_true_f,y_pred_f,n_classess_f) \n",
        "    r2_test       = r2_score(y_true_f, y_pred_f)\n",
        "    #mse_train = mean_squared_error(y_train, pred_train)\n",
        "    mse_test      = mean_squared_error(y_true_f, y_pred_f)\n",
        "    \n",
        "    score_table.loc[algo,:] = time, accuracy, Precision, Recall, F1,roc_auc, average_prec, r2_test, mse_test\n",
        "        \n",
        "    #print(classification_report(y_true_f,y_pred_f))\n",
        "    confusion_df = pd.DataFrame(confusion_matrix(y_true_f,y_pred_f),\n",
        "             columns=[\"Predicted Class \" + str(class_name) for class_name in target_namess_f],\n",
        "             index = [\"Class \" + str(class_name) for class_name in target_namess_f])\n",
        "\n",
        "    #print(confusion_df)\n",
        "    # Compute confusion matrix\n",
        "    cnf_matrix = confusion_matrix(y_true_f,y_pred_f)\n",
        "    np.set_printoptions(precision=2)\n",
        "\n",
        "    # Plot non-normalized confusion matrix\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix1(algo,cnf_matrix, y_true_f, y_pred_f, classes_f=target_namess_f, title='without normalization')\n",
        "\n",
        "    # Plot normalized confusion matrix\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix1(algo,cnf_matrix, y_true_f, y_pred_f, classes_f=target_namess_f, normalize=True, title='Normalized')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3KRquPpKFO1",
        "colab_type": "text"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F20fWGPtJ_3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize a score table to log the performance of various algorithms\n",
        "index = ['LogisticRegression','LinearDiscriminantAnalysis','KNeighborsClassifier','DecisionTreeClassifier','GaussianNB',\n",
        "         'SVC','RandomForestClassifier','XGBClassifier','AdaBoostClassifier','MLPClassifier','Deep Learning']\n",
        "score_table_2  = pd.DataFrame(index = index, columns= ['Time','Accuracy', 'Precision', 'Recall', 'F1','roc_auc_score','average_precision_score', 'r2_test', 'mse_test'])\n",
        "\n",
        "score_table_tot = pd.DataFrame(index = index, columns= ['Time','Accuracy', 'Precision', 'Recall', 'F1','roc_auc_score','average_precision_score', 'r2_test', 'mse_test'])\n",
        "\n",
        "score_table_3 = pd.DataFrame(index = index,columns= ['Parameters','Best parameters','Accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2F_OEESKJn7",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuN9ctzJ2Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "start_time = time.clock()\n",
        "\n",
        "# LASSO model with the new lag feature\n",
        "\n",
        "# initialize a Lasso model with default parameters\n",
        "linreg_2= Pipeline([('classifier',LogisticRegression())])\n",
        "\n",
        "# grid search of parameter alpha to improve lasso model\n",
        "parameters = [{'classifier__penalty': ['l1', 'l2'], 'classifier__C' : [1e2,1e5,1e6,1e8,1e9]}]\n",
        "linreg_2_cv = GridSearchCV(linreg_2, parameters, n_jobs=-1, scoring= 'accuracy',cv=10)\n",
        "linreg_2_cv.fit(X_train, y_train)\n",
        "#results.append(linreg_2_cv)\n",
        "print('Best parameters:', linreg_2_cv.best_params_)\n",
        "print('Corresponding accuracy score:', linreg_2_cv.best_score_)\n",
        "mean= linreg_2_cv.cv_results_['mean_test_score'][linreg_2_cv.best_index_]\n",
        "std = linreg_2_cv.cv_results_['std_test_score'][linreg_2_cv.best_index_]\n",
        "print(\"%f (%f)\" % (mean, std))\n",
        "\n",
        "\n",
        "algo='LogisticRegression'\n",
        "score_table_3.loc[algo,:] =parameters,linreg_2_cv.best_params_,(mean, std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlG4QOZ9J3LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple LinearRegression model with the new lag feature\n",
        "pen =linreg_2_cv.best_params_['classifier__penalty']\n",
        "cp  =linreg_2_cv.best_params_['classifier__C']\n",
        "\n",
        "linreg_2 = Pipeline([('scaler', StandardScaler()),('LogisticRegression',LogisticRegression(C=cp,penalty=pen))])\n",
        "linreg_2.fit(X_train, y_train)\n",
        "pred_train = linreg_2.predict(X_train)\n",
        "pred_test = linreg_2.predict(X_test)\n",
        "\n",
        "#feature_rank = pd.DataFrame({'feature':X_train.columns, 'coefficient':linreg_2.named_steps.linreg.coef_})\n",
        "#print(feature_rank.sort_values('coefficient',ascending=False))\n",
        "time1 = time.clock() - start_time\n",
        "compute_plot_result_3('LogisticRegression', time1, y_test, pred_test,n_classes,target_names,score_table_2)#y_true,y_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00K_3Vx8KMAg",
        "colab_type": "text"
      },
      "source": [
        "# Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZNXHxsKKQNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def highlight_max(data, color='green'):\n",
        "    '''\n",
        "    highlight the maximum in a Series or DataFrame\n",
        "    '''\n",
        "    attr = 'background-color: {}'.format(color)\n",
        "    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n",
        "        is_max = data == data.max()\n",
        "        return [attr if v else '' for v in is_max]\n",
        "    else:  # from .apply(axis=None)\n",
        "        is_max = data == data.max().max()\n",
        "        return pd.DataFrame(np.where(is_max, attr, ''), index=data.index, columns=data.columns)\n",
        "\n",
        "def highlight_min(data, color='red'):\n",
        "    '''\n",
        "    highlight the maximum in a Series or DataFrame\n",
        "    '''\n",
        "    attr = 'background-color: {}'.format(color)\n",
        "    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n",
        "        is_min = data == data.min()\n",
        "        return [attr if v else '' for v in is_min]\n",
        "    else:  # from .apply(axis=None)\n",
        "        is_min = data == data.min().min()\n",
        "        return pd.DataFrame(np.where(is_min, attr, ''), index=data.index, columns=data.columns)\n",
        "\n",
        "\n",
        "def draw_plot(data, edge_color, fill_color):\n",
        "    bp = ax.boxplot(data, patch_artist=True)\n",
        "    ax.set_xticklabels(data.index,rotation=90)\n",
        "    for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
        "        plt.setp(bp[element], color=edge_color)\n",
        "\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set(facecolor=fill_color)       \n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "draw_plot(score_table_3[\"Accuracy\"], 'red', 'tan')\n",
        "draw_plot(score_table_2[['Accuracy','Precision']], 'blue', 'cyan')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI8O_LoFKVns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_table_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcRKsmqGKd9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_table_2.style.apply(highlight_max).\\\n",
        "apply(highlight_min, color='darkorange',  subset=['Accuracy','Precision','Recall',\n",
        "                                                  'F1','r2_test','mse_test']).set_precision(4)\n",
        "\n",
        "#score_table_2.style.format({lambda x: \"{:.5f}\".format(x)})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
